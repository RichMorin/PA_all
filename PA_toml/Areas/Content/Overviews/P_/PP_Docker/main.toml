# con_ove|PP_Docker/main.toml

[ meta ]

  actions     = 'publish'
  id_str      = 'PP_Docker'
  title       = "Perkian - Docker"

[ meta.refs ]

  f_authors   = 'cat_peo|Rich_Morin'
  f_editors   = 'cat_peo|Rich_Morin'

[ about ]

  precis      = "Perkian implementation notes on Docker"

  verbose     = '''
Our current prototyping path concentrates
on [Linux]{ext_wp|Linux}-based [Docker]{ext_wp|Docker_(software)} containers,
which provide [OS-level virtualisation]{ext_wp|OS-level_virtualisation}
of many (but not all) runtime considerations.
Most Linux distributions are based on an official kernel
and use a common set of libraries, file system conventions, etc.
So, Linux-based containers can typically be run on any Linux system
that has the same processor architecture (e.g., 64-bit Intel).
That is, most desktop or laptop personal computers.

With a bit of work, these containers can also be run
on [Apple]{ext_wp|Apple_Inc}'s [macOS]{ext_wp|MacOS} or
[Microsoft Windows]{ext_wp|Microsoft_Windows}.
First, a [virtual machine]{ext_wp|Virtual_machine} platform
such as [VirtualBox]{ext_wp|VirtualBox} must be installed.
Then, a virtual machine must be created and run,
using a Linux distribution such as [Ubuntu]{ext_wp|Ubuntu}.
Things should get much simpler for Windows users
when "Windows Subsystem for Linux 2" is released.
Planned as part of an upcoming version of [Windows 10]{ext_wp|Windows_10},
WSL 2 will provide native support for Linux-based Docker containers.

Running Linux containers on a 32-bit ARM processor (e.g., a Raspberry Pi)
is also possible, but the containers will need to be built specifically
for this processor architecture.
Sadly, there does not appear to be a Docker-based solution
for most (e.g., [Android]{ext_wp|Android_(operating_system)},
[iOS]{ext_wp|IOS}.
So, for the moment, we are concentrating on 64-bit Intel machines,
but keeping 32-bit ARM processor (etc) in mind.

#### Docker Usage Differences

Docker is typically used by [DevOps]{ext_wp|DevOps} teams
to support well-defined (sets of) network services.
As a preliminary step, a number of smallish containers are created.
These are then combined (aka "orchestrated") to achieve a desired result.
This approach is extremely flexible and powerful,
but it doesn't match either our expected user base or usage modes.

Far from being DevOps experts, our users may be barely familiar with Linux.
Also, we can't know in advance what problems they will be trying to solve.
So, we need to build and use containers in a different manner.
Specifically, we assemble all of our add-on software
into a single, general-purpose container.

This lets the user treat the container as if it were simply another machine
on the local network.
This "machine" can provide web-based services,
run a large suite of commands, etc.
Conveniently, commands can be invoked and piped together
without concern for orchestration issues.
In addition, files can be read (or even written) on the host machine,
allowing a substantial degree of interoperability.

#### Building and Deployment

Docker containers can be built for several operating systems,
deployed via [Docker Hub]{https://hub.docker.com},
and downloaded via the Internet.
Although the container must match the downloading host's OS
and processor architecture, Docker's system of "manifests"
can be used to hide the gritty details.
So, the user (or software running on their behalf)
can request container "foo" and receive the appropriate version.

From the outside, a container acts a lot like an independent host,
connected via a network connection.
On the inside, it looks like some host OS, typically a flavor of Linux.
Although there are hundreds of Linux variants,
they share a common [OS kernel]{ext_wp|Kernel_(operating_system)}.
The [Debian]{ext_wp|Debian} family of Linux distributions, for instance,
is popular, robust, and has great build tooling.

If the desired host platform runs (or can emulate) a Linux variant,
we can create and deploy Debian-based containers
for each hardware platform we wish to support.
So, for example, we might only need to build containers for
32-bit ARM devices (e.g., the Raspberry Pi) and
64-bit Intel devices (e.g., typical personal computers).

The Docker build process is based on text-based
[Dockerfiles]{https://docs.docker.com/engine/reference/builder}.
These combine an overall syntax and structure
with OS-specific build commands.
Fortunately, a Unix-style "shell" and some common commands
are available for all the operating systems we have in mind.
So, a single Unixish Dockerfile should handle most of our targets.
Alternatively, we can abstract away some picky details
by using [Chef]{ext_wp|Chef_(software)} as high level build tooling.

Docker Hub and [GitHub]{ext_wp|GitHub} each provide
convenient and reliable ways to collect and distribute packages,
So, all we have to do is push our results and back away slowly.
The result (it says here :-) is a suite that can handle a range
of package building and distribution challenges.

#### Early Experiments

Our experimental build machine is a 2010 Mac Pro,
with lots of disk storage, memory, and processing cores.
As a preliminary, we've installed the following software stack:

- macOS
- VirtualBox
- Ubuntu Desktop
- Docker Engine
- Debian Container

We can log into the Ubuntu Desktop virtual machine (`make`)
from anywhere on our local area network ([LAN]{ext_wp|LAN}),
`sudo` to root, etc:

    $ ssh rdm@192.168.1.134
    ...
    rdm@make:~$ sudo -s
    rdm@make:~#

Let's start up a [bash]{ext_wp|Bash_(Unix_shell)} session
in the default [Debian container]{https://hub.docker.com/_/debian},
shorten the prompt string (`PS1`), and examine the runtime environment:

    rdm@make:~# docker run -it debian:latest bash
    ...
    root@d2e4f42055db:/# PS1='# '
    
    # cat /etc/os-release
    PRETTY_NAME="Debian GNU/Linux 10 (buster)"
    ...

    # ruby --version
    bash: ruby: command not found

This container doesn't include
[Ruby]{ext_wp|Ruby_(programming_language)}; let's install it:

    # apt update
    ...

    # apt-get -y install ruby-full
    ...

    # ruby --version
    ruby 2.5.5p157 ...

Now, in a different terminal, we can:

- Open a root shell on Ubuntu.
- Locate the running container.
- Commit it to an image (`deb-ruby`).
- Start up a new container and shell.
- Confirm that Ruby is still present.

Here goes...

    $ ssh rdm@192.168.1.134
    ...
    rdm@make:~$ sudo -s

    rdm@make:~# docker ps
    CONTAINER ID        IMAGE               ...
    d2e4f42055db        debian:latest       ...

    rdm@make:~# docker commit d2e4f42055db deb-ruby
    sha256:59d1e31...

    rdm@make:~# docker run -it deb-ruby bash
    root@0d40736e2bfd:/# ruby --version
    ruby 2.5.5p157 ...

#### Prototype

The example above demonstrates that we can create, load, and use
a custom Docker "image" containing a specified package.
However, we have several dozen packages that we'd like to incorporate
into a distributable Docker image.
Doing this in an interactive session would be both tedious and error prone.

So, we need a way to build an image in "batch mode",
collecting console output for later evaluation.
I'm currently prototyping a small set of control files and scripts for this:

- `add_ons`       - processes each entry in `add_ons.toml`
- `add_ons.toml`  - specifies package names and types
- `get_*`         - installs a specified package

To be continued...
'''

[ zoo ]

  snippets    = '...'
